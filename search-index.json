[{"content":"","date":"2023-09-07","id":0,"permalink":"/docs/about/","summary":"","tags":[],"title":"About"},{"content":"LeMaterial is an open-science initiative at the intersection of materials science and machine learning, created to accelerate the discovery of novel materials and deepen our understanding of the chemical space.\nOur mission is to build and maintain a suite of harmonised datasets, benchmark tasks, machine learning models, and collaborative tools that empower researchers across disciplines, from quantum chemistry and experimentalists to ML scientists and software engineers, to drive forward an open and more effective scientific ecosystem.\nWhy LeMaterial?\nThe materials science ecosystem is evolving rapidly, but remains deeply fragmented. Datasets are scattered across platforms, often inconsistently formatted or missing key metadata. Benchmarks, models, and tools are siloed, making it difficult to build on others‚Äô work or compare approaches fairly. LeMaterial aims to address this by creating a central, harmonized space where data, models, and ideas can move freely.\nLeMaterial aspires to be more than just a repository, more like a connective tissue for the community, bridging computational chemists, ML researchers, software engineers, and experimentalists. We believe progress happens when people with different expertise align early around shared challenges, not when they work in isolation.\nBy pooling skills, sharing infrastructure, and co-designing tools and standards, we hope to enable more ambitious, interdisciplinary science that no single group could tackle alone. LeMaterial offers early-career researchers a chance to contribute meaningfully, while empowering senior scientists to help shape foundational resources for the field.\nAbove all, our goal is to accelerate discovery while lowering barriers to entry ‚Äî so that any researcher with a good idea, regardless of background or affiliation, can help build the future of materials science\nWhat You Can Work On\nLeMaterial invites researchers from diverse domains to contribute to collaborative projects such as:\nIntegrating new datasets (e.g. crystals, defects, surfaces, reactions) and expanding the range of available properties Developing predictive and generative ML models for materials and molecules Expanding analysis tools to explore chemical and structural landscapes Designing and maintaining evaluation benchmarks and leaderboards ‚Ä¶and more! ü§ù¬†An Open Invitation to Collaborate\nLeMaterial is a platform for the community, built by the community. We welcome PhD students, postdocs, researchers, and professors to join forces on high-impact projects.\nOur collaborative working groups are open to all, and all datasets, tools, and models are released under permissive open-science licenses.\nWhile the initiative receives support from Entalpic and Hugging Face (for hosting, compute, and visibility), its direction is shaped by an open, distributed network of contributors.\nüì¨¬†Ready to Contribute?\nüëâ Join the LeMaterial Slack üëâ Suggest your own idea for LeMaterial Whether you‚Äôre contributing code, data, theory, or just curiosity ‚Äî we would love to have you involved.\nLet‚Äôs build the future of materials science ‚Äî together.\nFurther reading\nExplore our blogpost for more details on the LeMaterial! ","date":"2023-09-07","id":1,"permalink":"/docs/about/the-mission/","summary":"\u003cp\u003eLeMaterial is an open-science initiative at the intersection of materials science and machine learning, created to accelerate the discovery of novel materials and deepen our understanding of the chemical space.\u003c/p\u003e","tags":[],"title":"The mission"},{"content":"As part of LeMaterial, we released and will maintain different datasets, unifying and standardizing data from existing databases:\nLeMat-Bulk\nReleased in December 2024 This dataset unifies data from Materials Project, Alexandria, and OQMD into a high-quality resource with consistent and systematic properties (6,7M entries, 7 material properties) https://huggingface.co/datasets/LeMaterial/LeMat-Bulk LeMat-BulkUnique\nReleased in December 2024 This dataset provides de-duplicated material from Materials Project, Alexandria, and OQMD using our structure fingerprint algorithm. It is available in 3 subsets, for PBE, PBESol, and SCAN functionals https://huggingface.co/datasets/LeMaterial/LeMat-BulkUnique LeMat-Traj\nReleased in August 2025 LeMat-Traj provides a large-scale dataset, aggregating over 120 million atomic configurations of ab-initio relaxation trajectories, curated from multiple sources (MP, Alexandria, OQMD) and simulation protocols. It enables training and benchmarking of MLIPs and trajectory-aware models (e.g. force regressors, uncertainty quantifiers). https://huggingface.co/datasets/LeMaterial/LeMat-Traj https://arxiv.org/pdf/2508.20875 LeMat-Synth\nReleased in September 2025 LeMat-Synth is a multi-modal dataset that links materials, their synthesis procedures, and performance data. It was built by parsing scientific literature using VLM/LLM pipelines, and aims to support research on synthesisability prediction and planning. By analyzing over 80,000 open-access papers, LeMat-Synth builds one of the first large-scale datasets of material synthesis recipes, covering 35 synthesis methods and 16 material classes. https://huggingface.co/datasets/LeMaterial/LeMat-Synth https://www.arxiv.org/pdf/2510.26824 More datasets are under development, including surfaces, defects, and electron densities, and we welcome collaborators interested in extending or improving any of the above.\n","date":"2023-09-07","id":2,"permalink":"/docs/about/datasets/","summary":"\u003cp\u003eAs part of LeMaterial, we released and will maintain different datasets, unifying and standardizing data from existing databases:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eLeMat-Bulk\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReleased in December 2024\u003c/li\u003e\n\u003cli\u003eThis dataset unifies data from Materials Project, Alexandria, and OQMD into a high-quality resource with consistent and systematic properties (6,7M entries, 7 material properties)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://huggingface.co/datasets/LeMaterial/LeMat-Bulk\"\u003ehttps://huggingface.co/datasets/LeMaterial/LeMat-Bulk\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLeMat-BulkUnique\u003c/p\u003e","tags":[],"title":"Datasets"},{"content":" The goal of this tutorial is to show how to use LeMaterial\u0026rsquo;s dataset with Geometric GNNs designed for molecular property prediction and relaxation from the FAIRChem repository.\nFor more information on how to use FAIRChem\u0026rsquo;s models, please refer to the FAIRChem repository and their documentation.\nSetup the environment The best way to setup an environment for FAIRChem is to use the provided conda environment file and to create it with the following command:\nwget https://raw.githubusercontent.com/FAIR-Chem/fairchem/main/packages/env.gpu.yml conda env create -f env.gpu.yml conda activate fair-chem\rOr to separately install the torch_geometric dependencies:\n!pip install torch_geometric !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\rThen we need to install FAIRChem on the environment:\npip install fairchem-core\rOr manually (currently recommended way):\ngit clone https://github.com/FAIR-Chem/fairchem pip install -e fairchem/packages/fairchem-core\rWe also need to install PyTorch dependencies, make sure to pick the correct version of cuda for PyTorch Geometric, along with the right PyTorch version.\n!pip install fairchem-core !pip install torch_geometric !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.5.0+cu124.html !pip install datasets\rCPU = False # Run on CPU BATCH_SIZE = 2 # Train and evaluation batch size\rLoad the dataset We use the dataset available at LeMaterial\u0026rsquo;s Hugging Face space.\nfrom datasets import load_dataset HF_DATASET_PATH = \u0026#34;LeMaterial/LeMat-Bulk\u0026#34; SUBSET = \u0026#34;compatible_pbe\u0026#34; dataset = load_dataset(HF_DATASET_PATH, SUBSET)[\u0026#34;train\u0026#34;]\rDownloading data: 0%| | 0/17 [00:00\u0026lt;?, ?files/s] Generating train split: 0%| | 0/5335299 [00:00\u0026lt;?, ? examples/s] Loading dataset shards: 0%| | 0/17 [00:00\u0026lt;?, ?it/s] Resolving data files: 0%| | 0/17 [00:00\u0026lt;?, ?it/s] Load a model We need to start by loading a trained model on which we can run predictions. For example, we can download a checkpoint from EquiformerV2 available here.\nfrom huggingface_hub import hf_hub_download from fairchem.core import OCPCalculator HF_REPOID = \u0026#34;fairchem/OMAT24\u0026#34; HF_MODEL_PATH = \u0026#34;eqV2_31M_omat_mp_salex.pt\u0026#34; def download_model(hf_repo_id, hf_model_path): model_path = hf_hub_download(repo_id=hf_repo_id, filename=hf_model_path) return model_path model_path = download_model(HF_REPOID, HF_MODEL_PATH) calc = OCPCalculator(checkpoint_path=model_path, cpu=CPU)\r0it [00:00, ?it/s] eqV2_31M_omat_mp_salex.pt: 0%| | 0.00/126M [00:00\u0026lt;?, ?B/s] INFO:root:local rank base: 0 INFO:root:amp: true ... INFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run Inference on a single structure We first need to convert a row from the dataset of the material that we want to predict the property to an ASE molecule which can be digested by the model.\nfrom ase import Atoms from pymatgen.core.structure import Structure import numpy as np from collections import defaultdict from ase.calculators.singlepoint import SinglePointCalculator # To add targets from pymatgen.io.ase import AseAtomsAdaptor random_sample = np.random.randint(len(dataset)) row = dataset[random_sample] def get_atoms_from_row(row, add_targets=False, add_forces=True, add_stress=False): # Convert row to PyMatGen structure = Structure( [x for y in row[\u0026#34;lattice_vectors\u0026#34;] for x in y], species=row[\u0026#34;species_at_sites\u0026#34;], coords=row[\u0026#34;cartesian_site_positions\u0026#34;], coords_are_cartesian=True, ) atoms = AseAtomsAdaptor.get_atoms(structure) # Add the forces and energy as targets if add_targets: forces, stres = None, None if add_forces: if np.array(row[\u0026#34;forces\u0026#34;]).shape[0] == np.array(row[\u0026#34;cartesian_site_positions\u0026#34;]).shape[0]: forces = row[\u0026#34;forces\u0026#34;] else: return None # OMAT uses the stress tensor as output as well if add_stress: if np.array(row[\u0026#34;stress_tensor\u0026#34;]).shape[0] == np.array(row[\u0026#34;cartesian_site_positions\u0026#34;]).shape[0]: stress=row[\u0026#34;stress_tensor\u0026#34;] else: return None atoms.calc = SinglePointCalculator(atoms, forces=forces, energy=row[\u0026#34;energy\u0026#34;], stress=stress) return atoms atoms = get_atoms_from_row(row)\rWe can now run the inference on the chosen row of the dataset. Since most models inside FAIRChem are designed to predict the energy and the forces of a material at a given structure (S2EF), we can run relaxation (MD) on the structure to get the energy at the relaxed state as well.\nWe first show how to predict the energy property of a material without relaxation and then with relaxation.\nfrom ase.filters import FrechetCellFilter from ase.optimize import FIRE def relax_atoms(atoms, steps=0, fmax=0.05): atoms.calc = calc dyn = FIRE(FrechetCellFilter(atoms)) dyn.run(fmax=fmax, steps=steps) return atoms print(f\u0026#34;{\u0026#39;--\u0026#39; * 5} No Relaxation {\u0026#39;--\u0026#39; * 5}\u0026#34;) atoms = relax_atoms(atoms, steps=0) predicted_energy = atoms.get_potential_energy() print(f\u0026#34;Predicted energy: {predicted_energy} eV\u0026#34;) print(f\u0026#34;DFT energy: {row[\u0026#39;energy\u0026#39;]} eV\u0026#34;) print(\u0026#34;\\n\u0026#34;*2) print(f\u0026#34;{\u0026#39;--\u0026#39; * 5} With Relaxation {\u0026#39;--\u0026#39; * 5}\u0026#34;) atoms = get_atoms_from_row(row) atoms = relax_atoms(atoms, steps=200) predicted_energy = atoms.get_potential_energy() print(f\u0026#34;Predicted energy: {predicted_energy} eV\u0026#34;) print(f\u0026#34;DFT energy: {row[\u0026#39;energy\u0026#39;]} eV\u0026#34;)\r---------- No Relaxation ---------- Step Time Energy fmax FIRE: 0 14:54:04 -5.517979 0.267703 Predicted energy: -5.517978668212891 eV DFT energy: -5.57597026 eV ---------- With Relaxation ---------- Step Time Energy fmax FIRE: 0 14:54:04 -5.517979 0.267703 FIRE: 1 14:54:05 -5.520627 0.276172 FIRE: 2 14:54:05 -5.524797 0.263472 FIRE: 3 14:54:06 -5.527802 0.213701 FIRE: 4 14:54:06 -5.529005 0.166641 FIRE: 5 14:54:07 -5.529183 0.136835 FIRE: 6 14:54:07 -5.529638 0.124812 FIRE: 7 14:54:07 -5.531461 0.123644 FIRE: 8 14:54:08 -5.532405 0.125816 FIRE: 9 14:54:08 -5.534833 0.127752 FIRE: 10 14:54:08 -5.536007 0.130083 FIRE: 11 14:54:09 -5.535960 0.128654 FIRE: 12 14:54:09 -5.536138 0.120737 FIRE: 13 14:54:09 -5.533516 0.108630 FIRE: 14 14:54:10 -5.530128 0.100276 FIRE: 15 14:54:10 -5.528875 0.096984 FIRE: 16 14:54:10 -5.529051 0.104039 FIRE: 17 14:54:11 -5.570351 0.031070 Predicted energy: -5.5703511238098145 eV DFT energy: -5.57597026 eV Create an LMDB dataset In order to run batched inference, we need to create a database compatible with FAIRChem\u0026rsquo;s dataloader. The recommended way to do is to currently create an ASE LMDB database and pass it to FAIRChem\u0026rsquo;s config.\nfrom pathlib import Path import tqdm from ase import Atoms from fairchem.core.datasets.lmdb_database import LMDBDatabase\rWe will create an LMDB database based on LeMaterial\u0026rsquo;s entire dataset. Note that you can also use a subset of the dataset if you want to by filtering relevant structures for example. This could allow to fine-tune on selected materials, or test the model on a specific subset of materials.\nWe discuss about training and fine-tuning in the last section of this notebook.\n# REF: https://github.com/FAIR-Chem/fairchem/issues/787 output_path = Path(\u0026#34;leMat.aselmdb\u0026#34;) select_range = 100 small_dataset = dataset.select(range(select_range)) for row in tqdm.tqdm(small_dataset, total=len(small_dataset)): with LMDBDatabase(output_path) as db: atoms = get_atoms_from_row(row, add_targets=False) # not needed for inference db.write(atoms, data={\u0026#34;id\u0026#34;: row[\u0026#34;immutable_id\u0026#34;]})\r100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00\u0026lt;00:00, 118.21it/s] 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:01\u0026lt;00:00, 86.01it/s] Run batched inference Load the created LMDB dataset in model The model object loaded from the checkpoint contains all the necessary information on the config file, including the paths to the train, test and validation splits. We can use this information to load our newly created LeMaterial\u0026rsquo;s LMDB dataset to the model.\nfrom fairchem.core.common.tutorial_utils import generate_yml_config import yaml yml_path = generate_yml_config( model_path, \u0026#34;/tmp/config.yml\u0026#34;, delete=[ \u0026#34;logger\u0026#34;, \u0026#34;task\u0026#34;, \u0026#34;model_attributes\u0026#34;, \u0026#34;dataset\u0026#34;, \u0026#34;slurm\u0026#34;, \u0026#34;optim.load_balancing\u0026#34;, ], # Load balancing works only if a metadata.npz file is generated using the make_lmdb script (see: https://github.com/FAIR-Chem/fairchem/issues/876) update={ \u0026#34;amp\u0026#34;: True, \u0026#34;gpus\u0026#34;: 1, \u0026#34;task.prediction_dtype\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;tensorboard\u0026#34;, # Test data - prediction only so no regression \u0026#34;test_dataset.src\u0026#34;: \u0026#34;leMat.aselmdb\u0026#34;, \u0026#34;test_dataset.format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;test_dataset.2g_args.r_energy\u0026#34;: False, \u0026#34;test_dataset.a2g_args.r_forces\u0026#34;: False, \u0026#34;optim.eval_batch_size\u0026#34;: BATCH_SIZE, }, )\rINFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run /usr/local/lib/python3.10/dist-packages/fairchem/core/common/relaxation/ase_utils.py:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. checkpoint = torch.load(checkpoint_path, map_location=torch.device(\u0026quot;cpu\u0026quot;)) INFO:root:amp: true ... INFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run Run batched inference from fairchem.core.common.tutorial_utils import fairchem_main # Recommended way to run inference yml_path = generate_yml_config( model_path, \u0026#34;/tmp/config.yml\u0026#34;, delete=[ \u0026#34;logger\u0026#34;, \u0026#34;task\u0026#34;, \u0026#34;model_attributes\u0026#34;, \u0026#34;slurm\u0026#34;, \u0026#34;optim.load_balancing\u0026#34;, ], # Load balancing works only if a metadata.npz file is generated using the make_lmdb script (see: https://github.com/FAIR-Chem/fairchem/issues/876) update={ \u0026#34;amp\u0026#34;: True, \u0026#34;gpus\u0026#34;: 1, \u0026#34;task.prediction_dtype\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;tensorboard\u0026#34;, # Compatibility issues between current fairchem version and OMAT24 model? (not needed for inference) \u0026#34;loss_functionsn\u0026#34;: \u0026#34;mae\u0026#34;, # Test data - prediction only so no regression \u0026#34;test_dataset.src\u0026#34;: \u0026#34;leMat.aselmdb\u0026#34;, \u0026#34;test_dataset.format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;test_dataset.2g_args.r_energy\u0026#34;: False, \u0026#34;test_dataset.a2g_args.r_forces\u0026#34;: False, \u0026#34;optim.eval_batch_size\u0026#34;: BATCH_SIZE, }, ) import locale locale.getpreferredencoding = lambda: \u0026#34;UTF-8\u0026#34; # For running the main script !python {fairchem_main()} --mode predict --config-yml {yml_path} --checkpoint {model_path} {\u0026#39;--cpu\u0026#39; if CPU else \u0026#39;\u0026#39;}\r# If you want to have control over the trainer object (and for example add hooks on the modules) yml_path = generate_yml_config( model_path, \u0026#34;/tmp/config.yml\u0026#34;, delete=[ \u0026#34;logger\u0026#34;, \u0026#34;task\u0026#34;, \u0026#34;dataset\u0026#34; \u0026#34;model_attributes\u0026#34;, \u0026#34;slurm\u0026#34;, \u0026#34;optim.load_balancing\u0026#34;, ], # Load balancing works only if a metadata.npz file is generated using the make_lmdb script (see: https://github.com/FAIR-Chem/fairchem/issues/876) update={ \u0026#34;amp\u0026#34;: True, \u0026#34;gpus\u0026#34;: 1, \u0026#34;task.prediction_dtype\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;tensorboard\u0026#34;, # Test data - prediction only so no regression \u0026#34;test_dataset.src\u0026#34;: \u0026#34;leMat.aselmdb\u0026#34;, \u0026#34;test_dataset.format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;test_dataset.2g_args.r_energy\u0026#34;: False, \u0026#34;test_dataset.a2g_args.r_forces\u0026#34;: False, \u0026#34;optim.eval_batch_size\u0026#34;: BATCH_SIZE, }, ) config = yaml.safe_load(open(yml_path)) config[\u0026#34;dataset\u0026#34;] = {} config[\u0026#34;val_dataset\u0026#34;] = {} config[\u0026#34;optim\u0026#34;][\u0026#34;scheduler_params\u0026#34;] = {\u0026#39;lambda_type\u0026#39;: \u0026#39;cosine\u0026#39;, \u0026#39;warmup_factor\u0026#39;: 0.2, \u0026#39;warmup_epochs\u0026#39;: 463, \u0026#39;lr_min_factor\u0026#39;: 0.01, } calc.trainer.config = config calc.trainer.load_datasets() calc.trainer.is_debug = False calc.trainer.predict( calc.trainer.test_loader, calc.trainer.test_sampler )\rINFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run WARNING:root:Could not find dataset metadata.npz files in '[PosixPath('leMat.aselmdb')]' WARNING:root:Disabled BalancedBatchSampler because num_replicas=1. WARNING:root:Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. INFO:root:rank: 0: Sampler created... INFO:root:Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x794e8f76eec0\u0026gt;, batch_size=2, drop_last=False /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. warnings.warn( INFO:root:Predicting on test. device 0: 0%| | 0/50 [00:00\u0026lt;?, ?it/s]/usr/local/lib/python3.10/dist-packages/fairchem/core/trainers/ocp_trainer.py:471: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. with torch.cuda.amp.autocast(enabled=self.scaler is not None): device 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:13\u0026lt;00:00, 3.78it/s] INFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run WARNING:root:Could not find dataset metadata.npz files in '[PosixPath('leMat.aselmdb')]' WARNING:root:Disabled BalancedBatchSampler because num_replicas=1. WARNING:root:Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. INFO:root:rank: 0: Sampler created... INFO:root:Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x794e8f6f2260\u0026gt;, batch_size=2, drop_last=False /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. warnings.warn( INFO:root:Predicting on test. device 0: 0%| | 0/100 [00:00\u0026lt;?, ?it/s]/usr/local/lib/python3.10/dist-packages/fairchem/core/trainers/ocp_trainer.py:471: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. with torch.cuda.amp.autocast(enabled=self.scaler is not None): device 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:22\u0026lt;00:00, 4.35it/s] defaultdict(list, {'energy': [array([-92.6361], dtype=float32), array([-67.37538], dtype=float32), array([-46.047863], dtype=float32), array([-14.097839], dtype=float32), ... Train / fine-tune a model In order to train models with our dataset, we need to create the train and validation splits as well. This will require specifying the targets in the LMDB and letting the model correctly pick them up.\nSince LeMaterial\u0026rsquo;s database is composed of atomic forces and energies at a given structure (no trajectories for now), we want to use the energy and the forces as targets of an S2EF model. Note that EquiformerV2 trained on OMAT is an S2EFS (stress) model, so stress needs to be added to targets.\nIn order to train models with our dataset, we need to create the train and validation splits as well. This will require specifying the targets in the LMDB and letting the model correctly pick them up.\nWe provide an example of how it is possible to generate a few LMDB datasets and then plug use them for training a model. Notice that we need the targets in here which are directly read by the Atoms2Graph class internally.\nADD_STRESS = True # Need for omat24 models! (S2E\u0026#39;S) splits = { \u0026#34;train\u0026#34;: range(1000), \u0026#34;val\u0026#34;: range(1000, 2000), \u0026#34;test\u0026#34;: range(2000, 3000) } for split in splits: small_dataset = dataset.select(splits[split]) for row in tqdm.tqdm(small_dataset, total=len(small_dataset)): with LMDBDatabase(f\u0026#34;leMat_{split}.aselmdb\u0026#34;) as db: atoms = get_atoms_from_row(row, add_targets=True, add_forces=True, add_stress=ADD_STRESS) # Reject if there are no forces in the dataset if atoms is None: continue db.write(atoms, data={\u0026#34;id\u0026#34;: row[\u0026#34;immutable_id\u0026#34;]})\r100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02\u0026lt;00:00, 345.02it/s] 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02\u0026lt;00:00, 384.47it/s] 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01\u0026lt;00:00, 604.11it/s] 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03\u0026lt;00:00, 266.96it/s] 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02\u0026lt;00:00, 372.12it/s] 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01\u0026lt;00:00, 580.56it/s] Since creating LMDB files takes significantly more time as the size increases, we recommend separating the datasets into smaller chunks of .aselmdb files that are in the same directory and then redirect the config to this directory instead of the aselmdb file. The data loaders are then able to correctly concatenate the files as needed.\nMany model implementations exist in fairchem. This is an example of a few and how we use them. More of these can be found here.\nexample_configs = { \u0026#34;eqv2_omat_S\u0026#34;: \u0026#34;configs/omat24/all/eqV2_31M.yml\u0026#34;, \u0026#34;eqv2_omat_S\u0026#34;: \u0026#34;configs/omat24/all/eqV2_31M.yml\u0026#34;, \u0026#34;eqv2_s2ef_L\u0026#34;: \u0026#34;configs/s2ef/all/equiformer_v2/equiformer_v2_N@20_L@6_M@3_153M.yml\u0026#34;, \u0026#34;eqv2_s2ef_S\u0026#34;: \u0026#34;configs/s2ef/all/equiformer_v2/equiformer_v2_N@8_L@4_M@2_31M.yml\u0026#34;, \u0026#34;dpp\u0026#34;: \u0026#34;configs/s2ef/all/dimenet_plus_plus/dpp.yml\u0026#34;, } CHOSEN_MODEL = \u0026#34;dpp\u0026#34;\rWe need to apply a little bit of processing on the yaml config files to be able to read them with the main script of FAIRChem.\nApply the datasets Change some config parameters (this can be adjusted depending on what you want to put in the model) More information on how to tweak this config file can be found in FAIRChem\u0026rsquo;s documentation as well. Note that you can also modify some config arguments with the cli parameters directly.\nfrom fairchem.core.common.tutorial_utils import fairchem_main from pathlib import Path import yaml !git clone https://github.com/FAIR-Chem/fairchem.git # to download configs config_path = Path(\u0026#34;fairchem\u0026#34;) / example_configs[CHOSEN_MODEL] yaml_obj = yaml.load(open(config_path, \u0026#34;r\u0026#34;), Loader=yaml.FullLoader) # Change these parameters according to need include_dict = {\u0026#39;trainer\u0026#39;: \u0026#39;ocp\u0026#39;, \u0026#39;logger\u0026#39;: \u0026#39;tensorboard\u0026#39;, # or wandb \u0026#39;outputs\u0026#39;: {\u0026#39;energy\u0026#39;: {\u0026#39;shape\u0026#39;: 1, \u0026#39;level\u0026#39;: \u0026#39;system\u0026#39;}, \u0026#39;forces\u0026#39;: {\u0026#39;irrep_dim\u0026#39;: 1, \u0026#39;level\u0026#39;: \u0026#39;atom\u0026#39;, \u0026#39;train_on_free_atoms\u0026#39;: True, \u0026#39;eval_on_free_atoms\u0026#39;: True}}, \u0026#39;loss_functions\u0026#39;: [{\u0026#39;energy\u0026#39;: {\u0026#39;fn\u0026#39;: \u0026#39;mae\u0026#39;, \u0026#39;coefficient\u0026#39;: 1}}, {\u0026#39;forces\u0026#39;: {\u0026#39;fn\u0026#39;: \u0026#39;l2mae\u0026#39;, \u0026#39;coefficient\u0026#39;: 100}}], \u0026#39;evaluation_metrics\u0026#39;: {\u0026#39;metrics\u0026#39;: {\u0026#39;energy\u0026#39;: [\u0026#39;mae\u0026#39;], \u0026#39;forces\u0026#39;: [\u0026#39;mae\u0026#39;, \u0026#39;cosine_similarity\u0026#39;, \u0026#39;magnitude_error\u0026#39;], \u0026#39;misc\u0026#39;: [\u0026#39;energy_forces_within_threshold\u0026#39;]}, \u0026#39;primary_metric\u0026#39;: \u0026#39;forces_mae\u0026#39;} } yaml_obj[\u0026#34;dataset\u0026#34;] = { \u0026#34;train\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;leMat_train.aselmdb\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;a2g_args\u0026#34;: {\u0026#34;r_energy\u0026#34;: True, \u0026#34;r_forces\u0026#34;: True}}, \u0026#34;val\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;leMat_val.aselmdb\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;a2g_args\u0026#34;: {\u0026#34;r_energy\u0026#34;: True, \u0026#34;r_forces\u0026#34;: True}}, \u0026#34;test\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;leMat_test.aselmdb\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;a2g_args\u0026#34;: {\u0026#34;r_energy\u0026#34;: True, \u0026#34;r_forces\u0026#34;: True}}, } if \u0026#34;includes\u0026#34; in yaml_obj: del yaml_obj[\u0026#34;includes\u0026#34;] yaml_obj.update(include_dict) yaml_obj[\u0026#34;model\u0026#34;][\u0026#34;otf_graph\u0026#34;] = True # For equiformer models: set the trainer to the equiformerv2_forces one if \u0026#34;eqv2\u0026#34; in CHOSEN_MODEL: yaml_obj[\u0026#34;trainer\u0026#34;] = \u0026#34;equiformerv2_forces\u0026#34; yaml_obj[\u0026#34;optim\u0026#34;][\u0026#34;scheduler_params\u0026#34;] = {\u0026#39;lambda_type\u0026#39;: \u0026#39;cosine\u0026#39;, \u0026#39;warmup_factor\u0026#39;: 0.2, \u0026#39;warmup_epochs\u0026#39;: 463, \u0026#39;lr_min_factor\u0026#39;: 0.01, } # No metadata.npz file, disabling load_balancing if \u0026#34;load_balancing\u0026#34; in yaml_obj[\u0026#34;optim\u0026#34;]: del yaml_obj[\u0026#34;optim\u0026#34;][\u0026#34;load_balancing\u0026#34;] new_yaml_path = Path(f\u0026#34;/tmp/{CHOSEN_MODEL}_leMat.yml\u0026#34;) with open(new_yaml_path, \u0026#34;w\u0026#34;) as f: yaml.dump(yaml_obj, f)\rfatal: destination path 'fairchem' already exists and is not an empty directory. # Unclear whether it is possible to access this main.py script from the pip package !python fairchem/main.py --mode train --config-yml /tmp/{CHOSEN_MODEL}_leMat.yml {\u0026#34;--cpu\u0026#34; if CPU else \u0026#34;\u0026#34;}\r2024-12-12 14:38:02 (INFO): Running in local mode without elastic launch (single gpu only) 2024-12-12 14:38:02 (INFO): Setting env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True [W1212 14:38:02.705129468 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator()) 2024-12-12 14:38:02 (INFO): Project root: /usr/local/lib/python3.10/dist-packages/fairchem 2024-12-12 14:38:02.967649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2024-12-12 14:38:02.986891: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2024-12-12 14:38:02.992752: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 2024-12-12 14:38:04.287556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT 2024-12-12 14:38:05 (INFO): NumExpr defaulting to 2 threads. /usr/local/lib/python3.10/dist-packages/fairchem/core/models/scn/spherical_harmonics.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. _Jd = torch.load(os.path.join(os.path.dirname(__file__), \u0026quot;Jd.pt\u0026quot;)) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. _Jd = torch.load(os.path.join(os.path.dirname(__file__), \u0026quot;Jd.pt\u0026quot;)) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/layer_norm.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. @torch.cuda.amp.autocast(enabled=False) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/layer_norm.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. @torch.cuda.amp.autocast(enabled=False) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/layer_norm.py:263: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. @torch.cuda.amp.autocast(enabled=False) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/layer_norm.py:357: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. @torch.cuda.amp.autocast(enabled=False) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/escn/so3.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. _Jd = torch.load(os.path.join(os.path.dirname(__file__), \u0026quot;Jd.pt\u0026quot;)) 2024-12-12 14:38:07 (INFO): local rank base: 0 2024-12-12 14:38:07 (INFO): amp: false ... 2024-12-12 14:38:07 (INFO): Loading model: dimenetplusplus 2024-12-12 14:38:25 (INFO): Loaded DimeNetPlusPlusWrap with 1810182 parameters. 2024-12-12 14:38:25 (WARNING): log_summary for Tensorboard not supported 2024-12-12 14:38:25 (INFO): Loading dataset: ase_db 2024-12-12 14:38:25 (WARNING): Could not find dataset metadata.npz files in '[PosixPath('leMat_train.aselmdb')]' 2024-12-12 14:38:25 (WARNING): Disabled BalancedBatchSampler because num_replicas=1. 2024-12-12 14:38:25 (WARNING): Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. 2024-12-12 14:38:25 (INFO): rank: 0: Sampler created... 2024-12-12 14:38:25 (INFO): Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x7ed8d5dc2a70\u0026gt;, batch_size=8, drop_last=False /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. warnings.warn( 2024-12-12 14:38:25 (WARNING): Could not find dataset metadata.npz files in '[PosixPath('leMat_val.aselmdb')]' 2024-12-12 14:38:25 (WARNING): Disabled BalancedBatchSampler because num_replicas=1. 2024-12-12 14:38:25 (WARNING): Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. 2024-12-12 14:38:25 (INFO): rank: 0: Sampler created... 2024-12-12 14:38:25 (INFO): Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x7ed8d5b5b490\u0026gt;, batch_size=8, drop_last=False 2024-12-12 14:38:25 (WARNING): Could not find dataset metadata.npz files in '[PosixPath('leMat_test.aselmdb')]' 2024-12-12 14:38:25 (WARNING): Disabled BalancedBatchSampler because num_replicas=1. 2024-12-12 14:38:25 (WARNING): Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. 2024-12-12 14:38:25 (INFO): rank: 0: Sampler created... 2024-12-12 14:38:25 (INFO): Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x7ed8d5e109d0\u0026gt;, batch_size=8, drop_last=False /usr/local/lib/python3.10/dist-packages/fairchem/core/trainers/ocp_trainer.py:164: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. with torch.cuda.amp.autocast(enabled=self.scaler is not None): /usr/local/lib/python3.10/dist-packages/fairchem/core/models/dimenet_plus_plus.py:445: UserWarning: Using torch.cross without specifying the dim arg is deprecated. Please either pass the dim explicitly or simply use torch.linalg.cross. The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.) b = torch.cross(pos_ji, pos_kj).norm(dim=-1) /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed. This is not an error, but may impair performance. grad.sizes() = [1, 192], strides() = [1, 1] bucket_view.sizes() = [1, 192], strides() = [192, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.) return Variable._execution_engine.run_backward( # Calls into the C++ engine to run the backward pass 2024-12-12 14:38:29 (INFO): energy_mae: 1.66e+01, forces_mae: 8.61e-04, forces_cosine_similarity: 1.55e-01, forces_magnitude_error: 2.25e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.68e+01, lr: 2.00e-05, epoch: 9.09e-01, step: 1.00e+01 2024-12-12 14:38:32 (INFO): energy_mae: 1.75e+01, forces_mae: 6.67e-04, forces_cosine_similarity: 3.53e-01, forces_magnitude_error: 1.69e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.82e+01, lr: 2.00e-05, epoch: 1.82e+00, step: 2.00e+01 2024-12-12 14:38:36 (INFO): energy_mae: 1.65e+01, forces_mae: 8.29e-04, forces_cosine_similarity: 3.85e-01, forces_magnitude_error: 2.20e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.68e+01, lr: 2.00e-05, epoch: 2.73e+00, step: 3.00e+01 2024-12-12 14:38:39 (INFO): energy_mae: 1.64e+01, forces_mae: 4.72e-04, forces_cosine_similarity: 3.32e-01, forces_magnitude_error: 1.28e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.62e+01, lr: 2.00e-05, epoch: 3.64e+00, step: 4.00e+01 2024-12-12 14:38:42 (INFO): energy_mae: 1.67e+01, forces_mae: 5.07e-04, forces_cosine_similarity: 3.96e-01, forces_magnitude_error: 1.24e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.74e+01, lr: 2.00e-05, epoch: 4.55e+00, step: 5.00e+01 2024-12-12 14:38:45 (INFO): energy_mae: 1.78e+01, forces_mae: 5.62e-04, forces_cosine_similarity: 5.06e-01, forces_magnitude_error: 1.52e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.76e+01, lr: 2.00e-05, epoch: 5.45e+00, step: 6.00e+01 2024-12-12 14:38:47 (INFO): energy_mae: 1.60e+01, forces_mae: 4.84e-04, forces_cosine_similarity: 4.39e-01, forces_magnitude_error: 1.29e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.57e+01, lr: 2.00e-05, epoch: 6.36e+00, step: 7.00e+01 2024-12-12 14:38:49 (INFO): Total time taken: 23.739033460617065 Thanks to the FAIRChem team for their valuable feedback on this tutorial!\n","date":"2023-09-07","id":3,"permalink":"/docs/learn/lematerial-with-fairchem/","summary":"\u003ca target=\"_blank\" href=\"https://colab.research.google.com/drive/1y8_CzKM5Rgsiv9JoPmi9mXphi-kf6Lec?usp=sharing\"\u003e\n  \u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\n\u003c/a\u003e\n\u003cbr/\u003e\u003cbr/\u003e\n\u003cp\u003eThe goal of this tutorial is to show how to use LeMaterial\u0026rsquo;s dataset with Geometric GNNs designed for molecular property prediction and relaxation from the \u003ca href=\"https://github.com/FAIR-Chem/fairchem\"\u003eFAIRChem repository\u003c/a\u003e.\u003c/p\u003e","tags":[],"title":"LeMaterial with FAIRChem"},{"content":"All content released under the LeMaterial initiative is distributed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. This means the datasets and tools may be freely copied, distributed, modified, and used for derivative works, provided that appropriate credit is given to the LeMaterial project.\nIf you use LeMaterial in your research or software, please cite the data card citation section available on each dataset‚Äôs Hugging Face page. A consolidated reference paper is forthcoming.\nAttribution to Upstream Data Sources\nLeMaterial includes unified data from several foundational sources. Please make sure to also cite the original dataset(s) according to the provenance of the data you use.\nFor example in LeMat-Bulk, If you use materials data which include (‚Äùmp-‚Äù) in the immutable_id, please cite the¬†Materials Project. If they include (‚Äùagm-‚Äù) in the immutable_id, please cite¬†Alexandria PBE¬†or¬†Alexandria PBESol, SCAN. If they include (‚Äùoqmd-‚Äù) in the immutable_id, please cite¬†OQMD.\nTools Acknowledgement\nIf you make use of the Phase Diagram visualizer or the 3D crystal viewer in the LeMaterial Explorer, please acknowledge the Crystal Toolkit project.\n","date":"2023-09-07","id":4,"permalink":"/docs/about/citations/","summary":"\u003cp\u003eAll content released under the LeMaterial initiative is distributed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license. This means the datasets and tools may be freely copied, distributed, modified, and used for derivative works, provided that appropriate credit is given to the LeMaterial project.\u003c/p\u003e","tags":[],"title":"Citations"},{"content":"LeMaterial is an open and collaborative initiative ‚Äî and anyone with an interest in materials science, machine learning, or open scientific infrastructure is welcome to join. Whether you‚Äôre a student, researcher, engineer, or just curious, there‚Äôs a place for you in the community.\nThere are no strict requirements to join. We welcome contributors from all backgrounds ‚Äî including PhD students, postdocs, professors, independent researchers, software engineers, and experimentalists. If you are excited about the intersection of materials and machine learning, and want to collaborate, you are in the right place.\nTo get started:\nüëâ Fill out the short form here to introduce yourself and get access to collaborative updates. üëâ Join our Slack workspace here to connect with the community, join working groups, and stay up to date on ongoing projects. Google calendar link for the LeMaterial Reading Group Once you‚Äôre in, you‚Äôll be able to explore active projects, suggest new ideas, and find others to work with, or simply follow along and learn.\n","date":"2023-09-07","id":5,"permalink":"/docs/about/how-to-join/","summary":"\u003cp\u003e\u003cstrong\u003eLeMaterial is an open and collaborative initiative\u003c/strong\u003e ‚Äî and anyone with an interest in materials science, machine learning, or open scientific infrastructure is welcome to join. Whether you‚Äôre a student, researcher, engineer, or just curious, there‚Äôs a place for you in the community.\u003c/p\u003e","tags":[],"title":"How to join?"},{"content":"Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\nOur Standards Examples of behavior that contributes to a positive environment for our community include:\nDemonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include:\nThe use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others\u0026rsquo; private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\nScope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nEnforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at contact@lematerial.org. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\nEnforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n1. Correction Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n2. Warning Community Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n3. Temporary Ban Community Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n4. Permanent Ban Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\nAttribution This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla\u0026rsquo;s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.\n","date":"2023-09-07","id":6,"permalink":"/docs/about/code-of-conduct/","summary":"\u003ch2 id=\"our-pledge\"\u003eOur Pledge\u003c/h2\u003e\n\u003cp\u003eWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\u003c/p\u003e","tags":[],"title":"Code of Conduct"},{"content":"","date":"2025-02-06","id":7,"permalink":"/docs/learn/","summary":"","tags":[],"title":"Tutorials"},{"content":"","date":"2023-09-07","id":8,"permalink":"/docs/","summary":"","tags":[],"title":"About the project"},{"content":"","date":"2023-09-07","id":9,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"","date":"2023-09-07","id":10,"permalink":"/","summary":"","tags":[],"title":"LeMaterial"},{"content":"","date":"0001-01-01","id":11,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"0001-01-01","id":12,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"0001-01-01","id":13,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"}]