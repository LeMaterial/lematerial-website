[{"content":"","date":"2023-09-07","id":0,"permalink":"/docs/about/","summary":"","tags":[],"title":"About"},{"content":"LeMaterial is an open-science initiative dedicated to advancing materials research by providing harmonized data, useful tools, ML models, and various other collaborative resources. Our mission is to drive progress at the intersection of materials science and machine learning (ML), opening new opportunities to uncover novel materials and explore chemical spaces with unprecedented depth.\nIn materials science, the integration of ML with large databases of quantum chemical calculations has revolutionized high-throughput screening and accelerated the discovery of new materials. However, progress is often hindered by fragmented datasets that differ in format, scope, and parameters, making data integration and analysis challenging.\nThrough collaborative efforts, LeMaterial provides the largest harmonized dataset with compatible and standardized calculations, merging the most prominent material datasets, including Materials Project, Alexandria and OQMD. to deduplicate data and evaluate the novelty of generated materials.\nLeMaterial also proposed well-benchmarked hashing function to deduplicate data and evaluate the novelty of generated materials. These contributions serve as critical resources for the materials and AI4Science community.\nLeMaterial invites researchers from diverse domains to address those challenges and collaborate on further research topics such as :\nContributing to integrating new datasets (eg. trajectories, surfaces, reactions) and new properties Developing predictive and generative ML models, for various purposes Expanding analytical tools for chemical exploration Shaping evaluation benchmarks, such as leaderboards for generative models LeMaterial project operates in the spirit of Open Science. All datasets, models and tools are developed collectively and released under permissive licenses, ensuring accessibility to the entire community. While the project benefits from corporate support from Entalpic and Hugging Face (e.g. hosting datasets and compute), technical governance is driven by open working groups to ensure community collaboration and inclusivity.\nFurther reading Explore our blogpost for more details on the LeMaterial! ","date":"2023-09-07","id":1,"permalink":"/docs/about/the-mission/","summary":"\u003cp\u003e\u003cstrong\u003eLeMaterial\u003c/strong\u003e is an open-science initiative dedicated to advancing materials research by providing harmonized data, useful tools, ML models, and various other collaborative resources. Our mission is to drive progress at the intersection of materials science and machine learning (ML), opening new opportunities to uncover novel materials and explore chemical spaces with unprecedented depth.\u003c/p\u003e","tags":[],"title":"The mission"},{"content":"As part of LeMaterial, we released and will maintain different datasets, unifying and standardizing data from existing databases:\nLeMat-Bulk: released in December 2024, it unifies data from Materials Project, Alexandria, and OQMD into a high-quality resource with consistent and systematic properties (6,7M entries, 7 material properties)\nhttps://huggingface.co/datasets/LeMaterial/LeMat-Bulk LeMat-BulkUnique: released in December 2024, this dataset provides de-duplicated material from Materials Project, Alexandria, and OQMD using our structure fingerprint algorithm. It is available in 3 subsets, for PBE, PBESol, and SCAN functionals.\nhttps://huggingface.co/datasets/LeMaterial/LeMat-BulkUnique ","date":"2023-09-07","id":2,"permalink":"/docs/about/datasets/","summary":"\u003cp\u003eAs part of LeMaterial, we released and will maintain different datasets, unifying and standardizing data from existing databases:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eLeMat-Bulk\u003c/code\u003e: released in December 2024, it unifies data from Materials Project, Alexandria, and OQMD into a high-quality resource with consistent and systematic properties (6,7M entries, 7 material properties)\u003c/p\u003e","tags":[],"title":"Datasets"},{"content":" The goal of this tutorial is to show how to use LeMaterial\u0026rsquo;s dataset with Geometric GNNs designed for molecular property prediction and relaxation from the FAIRChem repository.\nFor more information on how to use FAIRChem\u0026rsquo;s models, please refer to the FAIRChem repository and their documentation.\nSetup the environment The best way to setup an environment for FAIRChem is to use the provided conda environment file and to create it with the following command:\nwget https://raw.githubusercontent.com/FAIR-Chem/fairchem/main/packages/env.gpu.yml conda env create -f env.gpu.yml conda activate fair-chem\rOr to separately install the torch_geometric dependencies:\n!pip install torch_geometric !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\rThen we need to install FAIRChem on the environment:\npip install fairchem-core\rOr manually (currently recommended way):\ngit clone https://github.com/FAIR-Chem/fairchem pip install -e fairchem/packages/fairchem-core\rWe also need to install PyTorch dependencies, make sure to pick the correct version of cuda for PyTorch Geometric, along with the right PyTorch version.\n!pip install fairchem-core !pip install torch_geometric !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.5.0+cu124.html !pip install datasets\rCPU = False # Run on CPU BATCH_SIZE = 2 # Train and evaluation batch size\rLoad the dataset We use the dataset available at LeMaterial\u0026rsquo;s Hugging Face space.\nfrom datasets import load_dataset HF_DATASET_PATH = \u0026#34;LeMaterial/LeMat-Bulk\u0026#34; SUBSET = \u0026#34;compatible_pbe\u0026#34; dataset = load_dataset(HF_DATASET_PATH, SUBSET)[\u0026#34;train\u0026#34;]\rDownloading data: 0%| | 0/17 [00:00\u0026lt;?, ?files/s] Generating train split: 0%| | 0/5335299 [00:00\u0026lt;?, ? examples/s] Loading dataset shards: 0%| | 0/17 [00:00\u0026lt;?, ?it/s] Resolving data files: 0%| | 0/17 [00:00\u0026lt;?, ?it/s] Load a model We need to start by loading a trained model on which we can run predictions. For example, we can download a checkpoint from EquiformerV2 available here.\nfrom huggingface_hub import hf_hub_download from fairchem.core import OCPCalculator HF_REPOID = \u0026#34;fairchem/OMAT24\u0026#34; HF_MODEL_PATH = \u0026#34;eqV2_31M_omat_mp_salex.pt\u0026#34; def download_model(hf_repo_id, hf_model_path): model_path = hf_hub_download(repo_id=hf_repo_id, filename=hf_model_path) return model_path model_path = download_model(HF_REPOID, HF_MODEL_PATH) calc = OCPCalculator(checkpoint_path=model_path, cpu=CPU)\r0it [00:00, ?it/s] eqV2_31M_omat_mp_salex.pt: 0%| | 0.00/126M [00:00\u0026lt;?, ?B/s] INFO:root:local rank base: 0 INFO:root:amp: true ... INFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run Inference on a single structure We first need to convert a row from the dataset of the material that we want to predict the property to an ASE molecule which can be digested by the model.\nfrom ase import Atoms from pymatgen.core.structure import Structure import numpy as np from collections import defaultdict from ase.calculators.singlepoint import SinglePointCalculator # To add targets from pymatgen.io.ase import AseAtomsAdaptor random_sample = np.random.randint(len(dataset)) row = dataset[random_sample] def get_atoms_from_row(row, add_targets=False, add_forces=True, add_stress=False): # Convert row to PyMatGen structure = Structure( [x for y in row[\u0026#34;lattice_vectors\u0026#34;] for x in y], species=row[\u0026#34;species_at_sites\u0026#34;], coords=row[\u0026#34;cartesian_site_positions\u0026#34;], coords_are_cartesian=True, ) atoms = AseAtomsAdaptor.get_atoms(structure) # Add the forces and energy as targets if add_targets: forces, stres = None, None if add_forces: if np.array(row[\u0026#34;forces\u0026#34;]).shape[0] == np.array(row[\u0026#34;cartesian_site_positions\u0026#34;]).shape[0]: forces = row[\u0026#34;forces\u0026#34;] else: return None # OMAT uses the stress tensor as output as well if add_stress: if np.array(row[\u0026#34;stress_tensor\u0026#34;]).shape[0] == np.array(row[\u0026#34;cartesian_site_positions\u0026#34;]).shape[0]: stress=row[\u0026#34;stress_tensor\u0026#34;] else: return None atoms.calc = SinglePointCalculator(atoms, forces=forces, energy=row[\u0026#34;energy\u0026#34;], stress=stress) return atoms atoms = get_atoms_from_row(row)\rWe can now run the inference on the chosen row of the dataset. Since most models inside FAIRChem are designed to predict the energy and the forces of a material at a given structure (S2EF), we can run relaxation (MD) on the structure to get the energy at the relaxed state as well.\nWe first show how to predict the energy property of a material without relaxation and then with relaxation.\nfrom ase.filters import FrechetCellFilter from ase.optimize import FIRE def relax_atoms(atoms, steps=0, fmax=0.05): atoms.calc = calc dyn = FIRE(FrechetCellFilter(atoms)) dyn.run(fmax=fmax, steps=steps) return atoms print(f\u0026#34;{\u0026#39;--\u0026#39; * 5} No Relaxation {\u0026#39;--\u0026#39; * 5}\u0026#34;) atoms = relax_atoms(atoms, steps=0) predicted_energy = atoms.get_potential_energy() print(f\u0026#34;Predicted energy: {predicted_energy} eV\u0026#34;) print(f\u0026#34;DFT energy: {row[\u0026#39;energy\u0026#39;]} eV\u0026#34;) print(\u0026#34;\\n\u0026#34;*2) print(f\u0026#34;{\u0026#39;--\u0026#39; * 5} With Relaxation {\u0026#39;--\u0026#39; * 5}\u0026#34;) atoms = get_atoms_from_row(row) atoms = relax_atoms(atoms, steps=200) predicted_energy = atoms.get_potential_energy() print(f\u0026#34;Predicted energy: {predicted_energy} eV\u0026#34;) print(f\u0026#34;DFT energy: {row[\u0026#39;energy\u0026#39;]} eV\u0026#34;)\r---------- No Relaxation ---------- Step Time Energy fmax FIRE: 0 14:54:04 -5.517979 0.267703 Predicted energy: -5.517978668212891 eV DFT energy: -5.57597026 eV ---------- With Relaxation ---------- Step Time Energy fmax FIRE: 0 14:54:04 -5.517979 0.267703 FIRE: 1 14:54:05 -5.520627 0.276172 FIRE: 2 14:54:05 -5.524797 0.263472 FIRE: 3 14:54:06 -5.527802 0.213701 FIRE: 4 14:54:06 -5.529005 0.166641 FIRE: 5 14:54:07 -5.529183 0.136835 FIRE: 6 14:54:07 -5.529638 0.124812 FIRE: 7 14:54:07 -5.531461 0.123644 FIRE: 8 14:54:08 -5.532405 0.125816 FIRE: 9 14:54:08 -5.534833 0.127752 FIRE: 10 14:54:08 -5.536007 0.130083 FIRE: 11 14:54:09 -5.535960 0.128654 FIRE: 12 14:54:09 -5.536138 0.120737 FIRE: 13 14:54:09 -5.533516 0.108630 FIRE: 14 14:54:10 -5.530128 0.100276 FIRE: 15 14:54:10 -5.528875 0.096984 FIRE: 16 14:54:10 -5.529051 0.104039 FIRE: 17 14:54:11 -5.570351 0.031070 Predicted energy: -5.5703511238098145 eV DFT energy: -5.57597026 eV Create an LMDB dataset In order to run batched inference, we need to create a database compatible with FAIRChem\u0026rsquo;s dataloader. The recommended way to do is to currently create an ASE LMDB database and pass it to FAIRChem\u0026rsquo;s config.\nfrom pathlib import Path import tqdm from ase import Atoms from fairchem.core.datasets.lmdb_database import LMDBDatabase\rWe will create an LMDB database based on LeMaterial\u0026rsquo;s entire dataset. Note that you can also use a subset of the dataset if you want to by filtering relevant structures for example. This could allow to fine-tune on selected materials, or test the model on a specific subset of materials.\nWe discuss about training and fine-tuning in the last section of this notebook.\n# REF: https://github.com/FAIR-Chem/fairchem/issues/787 output_path = Path(\u0026#34;leMat.aselmdb\u0026#34;) select_range = 100 small_dataset = dataset.select(range(select_range)) for row in tqdm.tqdm(small_dataset, total=len(small_dataset)): with LMDBDatabase(output_path) as db: atoms = get_atoms_from_row(row, add_targets=False) # not needed for inference db.write(atoms, data={\u0026#34;id\u0026#34;: row[\u0026#34;immutable_id\u0026#34;]})\r100%|██████████| 100/100 [00:00\u0026lt;00:00, 118.21it/s] 100%|██████████| 100/100 [00:01\u0026lt;00:00, 86.01it/s] Run batched inference Load the created LMDB dataset in model The model object loaded from the checkpoint contains all the necessary information on the config file, including the paths to the train, test and validation splits. We can use this information to load our newly created LeMaterial\u0026rsquo;s LMDB dataset to the model.\nfrom fairchem.core.common.tutorial_utils import generate_yml_config import yaml yml_path = generate_yml_config( model_path, \u0026#34;/tmp/config.yml\u0026#34;, delete=[ \u0026#34;logger\u0026#34;, \u0026#34;task\u0026#34;, \u0026#34;model_attributes\u0026#34;, \u0026#34;dataset\u0026#34;, \u0026#34;slurm\u0026#34;, \u0026#34;optim.load_balancing\u0026#34;, ], # Load balancing works only if a metadata.npz file is generated using the make_lmdb script (see: https://github.com/FAIR-Chem/fairchem/issues/876) update={ \u0026#34;amp\u0026#34;: True, \u0026#34;gpus\u0026#34;: 1, \u0026#34;task.prediction_dtype\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;tensorboard\u0026#34;, # Test data - prediction only so no regression \u0026#34;test_dataset.src\u0026#34;: \u0026#34;leMat.aselmdb\u0026#34;, \u0026#34;test_dataset.format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;test_dataset.2g_args.r_energy\u0026#34;: False, \u0026#34;test_dataset.a2g_args.r_forces\u0026#34;: False, \u0026#34;optim.eval_batch_size\u0026#34;: BATCH_SIZE, }, )\rINFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run /usr/local/lib/python3.10/dist-packages/fairchem/core/common/relaxation/ase_utils.py:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. checkpoint = torch.load(checkpoint_path, map_location=torch.device(\u0026quot;cpu\u0026quot;)) INFO:root:amp: true ... INFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run Run batched inference from fairchem.core.common.tutorial_utils import fairchem_main # Recommended way to run inference yml_path = generate_yml_config( model_path, \u0026#34;/tmp/config.yml\u0026#34;, delete=[ \u0026#34;logger\u0026#34;, \u0026#34;task\u0026#34;, \u0026#34;model_attributes\u0026#34;, \u0026#34;slurm\u0026#34;, \u0026#34;optim.load_balancing\u0026#34;, ], # Load balancing works only if a metadata.npz file is generated using the make_lmdb script (see: https://github.com/FAIR-Chem/fairchem/issues/876) update={ \u0026#34;amp\u0026#34;: True, \u0026#34;gpus\u0026#34;: 1, \u0026#34;task.prediction_dtype\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;tensorboard\u0026#34;, # Compatibility issues between current fairchem version and OMAT24 model? (not needed for inference) \u0026#34;loss_functionsn\u0026#34;: \u0026#34;mae\u0026#34;, # Test data - prediction only so no regression \u0026#34;test_dataset.src\u0026#34;: \u0026#34;leMat.aselmdb\u0026#34;, \u0026#34;test_dataset.format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;test_dataset.2g_args.r_energy\u0026#34;: False, \u0026#34;test_dataset.a2g_args.r_forces\u0026#34;: False, \u0026#34;optim.eval_batch_size\u0026#34;: BATCH_SIZE, }, ) import locale locale.getpreferredencoding = lambda: \u0026#34;UTF-8\u0026#34; # For running the main script !python {fairchem_main()} --mode predict --config-yml {yml_path} --checkpoint {model_path} {\u0026#39;--cpu\u0026#39; if CPU else \u0026#39;\u0026#39;}\r# If you want to have control over the trainer object (and for example add hooks on the modules) yml_path = generate_yml_config( model_path, \u0026#34;/tmp/config.yml\u0026#34;, delete=[ \u0026#34;logger\u0026#34;, \u0026#34;task\u0026#34;, \u0026#34;dataset\u0026#34; \u0026#34;model_attributes\u0026#34;, \u0026#34;slurm\u0026#34;, \u0026#34;optim.load_balancing\u0026#34;, ], # Load balancing works only if a metadata.npz file is generated using the make_lmdb script (see: https://github.com/FAIR-Chem/fairchem/issues/876) update={ \u0026#34;amp\u0026#34;: True, \u0026#34;gpus\u0026#34;: 1, \u0026#34;task.prediction_dtype\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;logger\u0026#34;: \u0026#34;tensorboard\u0026#34;, # Test data - prediction only so no regression \u0026#34;test_dataset.src\u0026#34;: \u0026#34;leMat.aselmdb\u0026#34;, \u0026#34;test_dataset.format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;test_dataset.2g_args.r_energy\u0026#34;: False, \u0026#34;test_dataset.a2g_args.r_forces\u0026#34;: False, \u0026#34;optim.eval_batch_size\u0026#34;: BATCH_SIZE, }, ) config = yaml.safe_load(open(yml_path)) config[\u0026#34;dataset\u0026#34;] = {} config[\u0026#34;val_dataset\u0026#34;] = {} config[\u0026#34;optim\u0026#34;][\u0026#34;scheduler_params\u0026#34;] = {\u0026#39;lambda_type\u0026#39;: \u0026#39;cosine\u0026#39;, \u0026#39;warmup_factor\u0026#39;: 0.2, \u0026#39;warmup_epochs\u0026#39;: 463, \u0026#39;lr_min_factor\u0026#39;: 0.01, } calc.trainer.config = config calc.trainer.load_datasets() calc.trainer.is_debug = False calc.trainer.predict( calc.trainer.test_loader, calc.trainer.test_sampler )\rINFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run WARNING:root:Could not find dataset metadata.npz files in '[PosixPath('leMat.aselmdb')]' WARNING:root:Disabled BalancedBatchSampler because num_replicas=1. WARNING:root:Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. INFO:root:rank: 0: Sampler created... INFO:root:Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x794e8f76eec0\u0026gt;, batch_size=2, drop_last=False /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. warnings.warn( INFO:root:Predicting on test. device 0: 0%| | 0/50 [00:00\u0026lt;?, ?it/s]/usr/local/lib/python3.10/dist-packages/fairchem/core/trainers/ocp_trainer.py:471: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. with torch.cuda.amp.autocast(enabled=self.scaler is not None): device 0: 100%|██████████| 50/50 [00:13\u0026lt;00:00, 3.78it/s] INFO:root:Loading model: hydra WARNING:root:equiformerV2_energy_head (EquiformerV2EnergyHead) class is deprecated in favor of equiformerV2_scalar_head (EqV2ScalarHead) WARNING:root:equiformerV2_force_head (EquiformerV2ForceHead) class is deprecated in favor of equiformerV2_rank1_head (EqV2Rank1Head) INFO:root:Loaded HydraModel with 31207434 parameters. INFO:root:Loading checkpoint in inference-only mode, not loading keys associated with trainer state! WARNING:root:No seed has been set in modelcheckpoint or OCPCalculator! Results may not be reproducible on re-run WARNING:root:Could not find dataset metadata.npz files in '[PosixPath('leMat.aselmdb')]' WARNING:root:Disabled BalancedBatchSampler because num_replicas=1. WARNING:root:Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. INFO:root:rank: 0: Sampler created... INFO:root:Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x794e8f6f2260\u0026gt;, batch_size=2, drop_last=False /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. warnings.warn( INFO:root:Predicting on test. device 0: 0%| | 0/100 [00:00\u0026lt;?, ?it/s]/usr/local/lib/python3.10/dist-packages/fairchem/core/trainers/ocp_trainer.py:471: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. with torch.cuda.amp.autocast(enabled=self.scaler is not None): device 0: 100%|██████████| 100/100 [00:22\u0026lt;00:00, 4.35it/s] defaultdict(list, {'energy': [array([-92.6361], dtype=float32), array([-67.37538], dtype=float32), array([-46.047863], dtype=float32), array([-14.097839], dtype=float32), ... Train / fine-tune a model In order to train models with our dataset, we need to create the train and validation splits as well. This will require specifying the targets in the LMDB and letting the model correctly pick them up.\nSince LeMaterial\u0026rsquo;s database is composed of atomic forces and energies at a given structure (no trajectories for now), we want to use the energy and the forces as targets of an S2EF model. Note that EquiformerV2 trained on OMAT is an S2EFS (stress) model, so stress needs to be added to targets.\nIn order to train models with our dataset, we need to create the train and validation splits as well. This will require specifying the targets in the LMDB and letting the model correctly pick them up.\nWe provide an example of how it is possible to generate a few LMDB datasets and then plug use them for training a model. Notice that we need the targets in here which are directly read by the Atoms2Graph class internally.\nADD_STRESS = True # Need for omat24 models! (S2E\u0026#39;S) splits = { \u0026#34;train\u0026#34;: range(1000), \u0026#34;val\u0026#34;: range(1000, 2000), \u0026#34;test\u0026#34;: range(2000, 3000) } for split in splits: small_dataset = dataset.select(splits[split]) for row in tqdm.tqdm(small_dataset, total=len(small_dataset)): with LMDBDatabase(f\u0026#34;leMat_{split}.aselmdb\u0026#34;) as db: atoms = get_atoms_from_row(row, add_targets=True, add_forces=True, add_stress=ADD_STRESS) # Reject if there are no forces in the dataset if atoms is None: continue db.write(atoms, data={\u0026#34;id\u0026#34;: row[\u0026#34;immutable_id\u0026#34;]})\r100%|██████████| 1000/1000 [00:02\u0026lt;00:00, 345.02it/s] 100%|██████████| 1000/1000 [00:02\u0026lt;00:00, 384.47it/s] 100%|██████████| 1000/1000 [00:01\u0026lt;00:00, 604.11it/s] 100%|██████████| 1000/1000 [00:03\u0026lt;00:00, 266.96it/s] 100%|██████████| 1000/1000 [00:02\u0026lt;00:00, 372.12it/s] 100%|██████████| 1000/1000 [00:01\u0026lt;00:00, 580.56it/s] Since creating LMDB files takes significantly more time as the size increases, we recommend separating the datasets into smaller chunks of .aselmdb files that are in the same directory and then redirect the config to this directory instead of the aselmdb file. The data loaders are then able to correctly concatenate the files as needed.\nMany model implementations exist in fairchem. This is an example of a few and how we use them. More of these can be found here.\nexample_configs = { \u0026#34;eqv2_omat_S\u0026#34;: \u0026#34;configs/omat24/all/eqV2_31M.yml\u0026#34;, \u0026#34;eqv2_omat_S\u0026#34;: \u0026#34;configs/omat24/all/eqV2_31M.yml\u0026#34;, \u0026#34;eqv2_s2ef_L\u0026#34;: \u0026#34;configs/s2ef/all/equiformer_v2/equiformer_v2_N@20_L@6_M@3_153M.yml\u0026#34;, \u0026#34;eqv2_s2ef_S\u0026#34;: \u0026#34;configs/s2ef/all/equiformer_v2/equiformer_v2_N@8_L@4_M@2_31M.yml\u0026#34;, \u0026#34;dpp\u0026#34;: \u0026#34;configs/s2ef/all/dimenet_plus_plus/dpp.yml\u0026#34;, } CHOSEN_MODEL = \u0026#34;dpp\u0026#34;\rWe need to apply a little bit of processing on the yaml config files to be able to read them with the main script of FAIRChem.\nApply the datasets Change some config parameters (this can be adjusted depending on what you want to put in the model) More information on how to tweak this config file can be found in FAIRChem\u0026rsquo;s documentation as well. Note that you can also modify some config arguments with the cli parameters directly.\nfrom fairchem.core.common.tutorial_utils import fairchem_main from pathlib import Path import yaml !git clone https://github.com/FAIR-Chem/fairchem.git # to download configs config_path = Path(\u0026#34;fairchem\u0026#34;) / example_configs[CHOSEN_MODEL] yaml_obj = yaml.load(open(config_path, \u0026#34;r\u0026#34;), Loader=yaml.FullLoader) # Change these parameters according to need include_dict = {\u0026#39;trainer\u0026#39;: \u0026#39;ocp\u0026#39;, \u0026#39;logger\u0026#39;: \u0026#39;tensorboard\u0026#39;, # or wandb \u0026#39;outputs\u0026#39;: {\u0026#39;energy\u0026#39;: {\u0026#39;shape\u0026#39;: 1, \u0026#39;level\u0026#39;: \u0026#39;system\u0026#39;}, \u0026#39;forces\u0026#39;: {\u0026#39;irrep_dim\u0026#39;: 1, \u0026#39;level\u0026#39;: \u0026#39;atom\u0026#39;, \u0026#39;train_on_free_atoms\u0026#39;: True, \u0026#39;eval_on_free_atoms\u0026#39;: True}}, \u0026#39;loss_functions\u0026#39;: [{\u0026#39;energy\u0026#39;: {\u0026#39;fn\u0026#39;: \u0026#39;mae\u0026#39;, \u0026#39;coefficient\u0026#39;: 1}}, {\u0026#39;forces\u0026#39;: {\u0026#39;fn\u0026#39;: \u0026#39;l2mae\u0026#39;, \u0026#39;coefficient\u0026#39;: 100}}], \u0026#39;evaluation_metrics\u0026#39;: {\u0026#39;metrics\u0026#39;: {\u0026#39;energy\u0026#39;: [\u0026#39;mae\u0026#39;], \u0026#39;forces\u0026#39;: [\u0026#39;mae\u0026#39;, \u0026#39;cosine_similarity\u0026#39;, \u0026#39;magnitude_error\u0026#39;], \u0026#39;misc\u0026#39;: [\u0026#39;energy_forces_within_threshold\u0026#39;]}, \u0026#39;primary_metric\u0026#39;: \u0026#39;forces_mae\u0026#39;} } yaml_obj[\u0026#34;dataset\u0026#34;] = { \u0026#34;train\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;leMat_train.aselmdb\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;a2g_args\u0026#34;: {\u0026#34;r_energy\u0026#34;: True, \u0026#34;r_forces\u0026#34;: True}}, \u0026#34;val\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;leMat_val.aselmdb\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;a2g_args\u0026#34;: {\u0026#34;r_energy\u0026#34;: True, \u0026#34;r_forces\u0026#34;: True}}, \u0026#34;test\u0026#34;: {\u0026#34;src\u0026#34;: \u0026#34;leMat_test.aselmdb\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;ase_db\u0026#34;, \u0026#34;a2g_args\u0026#34;: {\u0026#34;r_energy\u0026#34;: True, \u0026#34;r_forces\u0026#34;: True}}, } if \u0026#34;includes\u0026#34; in yaml_obj: del yaml_obj[\u0026#34;includes\u0026#34;] yaml_obj.update(include_dict) yaml_obj[\u0026#34;model\u0026#34;][\u0026#34;otf_graph\u0026#34;] = True # For equiformer models: set the trainer to the equiformerv2_forces one if \u0026#34;eqv2\u0026#34; in CHOSEN_MODEL: yaml_obj[\u0026#34;trainer\u0026#34;] = \u0026#34;equiformerv2_forces\u0026#34; yaml_obj[\u0026#34;optim\u0026#34;][\u0026#34;scheduler_params\u0026#34;] = {\u0026#39;lambda_type\u0026#39;: \u0026#39;cosine\u0026#39;, \u0026#39;warmup_factor\u0026#39;: 0.2, \u0026#39;warmup_epochs\u0026#39;: 463, \u0026#39;lr_min_factor\u0026#39;: 0.01, } # No metadata.npz file, disabling load_balancing if \u0026#34;load_balancing\u0026#34; in yaml_obj[\u0026#34;optim\u0026#34;]: del yaml_obj[\u0026#34;optim\u0026#34;][\u0026#34;load_balancing\u0026#34;] new_yaml_path = Path(f\u0026#34;/tmp/{CHOSEN_MODEL}_leMat.yml\u0026#34;) with open(new_yaml_path, \u0026#34;w\u0026#34;) as f: yaml.dump(yaml_obj, f)\rfatal: destination path 'fairchem' already exists and is not an empty directory. # Unclear whether it is possible to access this main.py script from the pip package !python fairchem/main.py --mode train --config-yml /tmp/{CHOSEN_MODEL}_leMat.yml {\u0026#34;--cpu\u0026#34; if CPU else \u0026#34;\u0026#34;}\r2024-12-12 14:38:02 (INFO): Running in local mode without elastic launch (single gpu only) 2024-12-12 14:38:02 (INFO): Setting env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True [W1212 14:38:02.705129468 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator()) 2024-12-12 14:38:02 (INFO): Project root: /usr/local/lib/python3.10/dist-packages/fairchem 2024-12-12 14:38:02.967649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered 2024-12-12 14:38:02.986891: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered 2024-12-12 14:38:02.992752: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered 2024-12-12 14:38:04.287556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT 2024-12-12 14:38:05 (INFO): NumExpr defaulting to 2 threads. /usr/local/lib/python3.10/dist-packages/fairchem/core/models/scn/spherical_harmonics.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. _Jd = torch.load(os.path.join(os.path.dirname(__file__), \u0026quot;Jd.pt\u0026quot;)) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. _Jd = torch.load(os.path.join(os.path.dirname(__file__), \u0026quot;Jd.pt\u0026quot;)) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/layer_norm.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. @torch.cuda.amp.autocast(enabled=False) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/layer_norm.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. @torch.cuda.amp.autocast(enabled=False) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/layer_norm.py:263: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. @torch.cuda.amp.autocast(enabled=False) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/equiformer_v2/layer_norm.py:357: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. @torch.cuda.amp.autocast(enabled=False) /usr/local/lib/python3.10/dist-packages/fairchem/core/models/escn/so3.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. _Jd = torch.load(os.path.join(os.path.dirname(__file__), \u0026quot;Jd.pt\u0026quot;)) 2024-12-12 14:38:07 (INFO): local rank base: 0 2024-12-12 14:38:07 (INFO): amp: false ... 2024-12-12 14:38:07 (INFO): Loading model: dimenetplusplus 2024-12-12 14:38:25 (INFO): Loaded DimeNetPlusPlusWrap with 1810182 parameters. 2024-12-12 14:38:25 (WARNING): log_summary for Tensorboard not supported 2024-12-12 14:38:25 (INFO): Loading dataset: ase_db 2024-12-12 14:38:25 (WARNING): Could not find dataset metadata.npz files in '[PosixPath('leMat_train.aselmdb')]' 2024-12-12 14:38:25 (WARNING): Disabled BalancedBatchSampler because num_replicas=1. 2024-12-12 14:38:25 (WARNING): Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. 2024-12-12 14:38:25 (INFO): rank: 0: Sampler created... 2024-12-12 14:38:25 (INFO): Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x7ed8d5dc2a70\u0026gt;, batch_size=8, drop_last=False /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. warnings.warn( 2024-12-12 14:38:25 (WARNING): Could not find dataset metadata.npz files in '[PosixPath('leMat_val.aselmdb')]' 2024-12-12 14:38:25 (WARNING): Disabled BalancedBatchSampler because num_replicas=1. 2024-12-12 14:38:25 (WARNING): Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. 2024-12-12 14:38:25 (INFO): rank: 0: Sampler created... 2024-12-12 14:38:25 (INFO): Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x7ed8d5b5b490\u0026gt;, batch_size=8, drop_last=False 2024-12-12 14:38:25 (WARNING): Could not find dataset metadata.npz files in '[PosixPath('leMat_test.aselmdb')]' 2024-12-12 14:38:25 (WARNING): Disabled BalancedBatchSampler because num_replicas=1. 2024-12-12 14:38:25 (WARNING): Failed to get data sizes, falling back to uniform partitioning. BalancedBatchSampler requires a dataset that has a metadata attributed with number of atoms. 2024-12-12 14:38:25 (INFO): rank: 0: Sampler created... 2024-12-12 14:38:25 (INFO): Created BalancedBatchSampler with sampler=\u0026lt;fairchem.core.common.data_parallel.StatefulDistributedSampler object at 0x7ed8d5e109d0\u0026gt;, batch_size=8, drop_last=False /usr/local/lib/python3.10/dist-packages/fairchem/core/trainers/ocp_trainer.py:164: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead. with torch.cuda.amp.autocast(enabled=self.scaler is not None): /usr/local/lib/python3.10/dist-packages/fairchem/core/models/dimenet_plus_plus.py:445: UserWarning: Using torch.cross without specifying the dim arg is deprecated. Please either pass the dim explicitly or simply use torch.linalg.cross. The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.) b = torch.cross(pos_ji, pos_kj).norm(dim=-1) /usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed. This is not an error, but may impair performance. grad.sizes() = [1, 192], strides() = [1, 1] bucket_view.sizes() = [1, 192], strides() = [192, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.) return Variable._execution_engine.run_backward( # Calls into the C++ engine to run the backward pass 2024-12-12 14:38:29 (INFO): energy_mae: 1.66e+01, forces_mae: 8.61e-04, forces_cosine_similarity: 1.55e-01, forces_magnitude_error: 2.25e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.68e+01, lr: 2.00e-05, epoch: 9.09e-01, step: 1.00e+01 2024-12-12 14:38:32 (INFO): energy_mae: 1.75e+01, forces_mae: 6.67e-04, forces_cosine_similarity: 3.53e-01, forces_magnitude_error: 1.69e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.82e+01, lr: 2.00e-05, epoch: 1.82e+00, step: 2.00e+01 2024-12-12 14:38:36 (INFO): energy_mae: 1.65e+01, forces_mae: 8.29e-04, forces_cosine_similarity: 3.85e-01, forces_magnitude_error: 2.20e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.68e+01, lr: 2.00e-05, epoch: 2.73e+00, step: 3.00e+01 2024-12-12 14:38:39 (INFO): energy_mae: 1.64e+01, forces_mae: 4.72e-04, forces_cosine_similarity: 3.32e-01, forces_magnitude_error: 1.28e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.62e+01, lr: 2.00e-05, epoch: 3.64e+00, step: 4.00e+01 2024-12-12 14:38:42 (INFO): energy_mae: 1.67e+01, forces_mae: 5.07e-04, forces_cosine_similarity: 3.96e-01, forces_magnitude_error: 1.24e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.74e+01, lr: 2.00e-05, epoch: 4.55e+00, step: 5.00e+01 2024-12-12 14:38:45 (INFO): energy_mae: 1.78e+01, forces_mae: 5.62e-04, forces_cosine_similarity: 5.06e-01, forces_magnitude_error: 1.52e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.76e+01, lr: 2.00e-05, epoch: 5.45e+00, step: 6.00e+01 2024-12-12 14:38:47 (INFO): energy_mae: 1.60e+01, forces_mae: 4.84e-04, forces_cosine_similarity: 4.39e-01, forces_magnitude_error: 1.29e-03, energy_forces_within_threshold: 0.00e+00, loss: 1.57e+01, lr: 2.00e-05, epoch: 6.36e+00, step: 7.00e+01 2024-12-12 14:38:49 (INFO): Total time taken: 23.739033460617065 Thanks to the FAIRChem team for their valuable feedback on this tutorial!\n","date":"2023-09-07","id":3,"permalink":"/docs/learn/lematerial-with-fairchem/","summary":"\u003ca target=\"_blank\" href=\"https://colab.research.google.com/drive/1y8_CzKM5Rgsiv9JoPmi9mXphi-kf6Lec?usp=sharing\"\u003e\n  \u003cimg src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/\u003e\n\u003c/a\u003e\n\u003cbr/\u003e\u003cbr/\u003e\n\u003cp\u003eThe goal of this tutorial is to show how to use LeMaterial\u0026rsquo;s dataset with Geometric GNNs designed for molecular property prediction and relaxation from the \u003ca href=\"https://github.com/FAIR-Chem/fairchem\"\u003eFAIRChem repository\u003c/a\u003e.\u003c/p\u003e","tags":[],"title":"LeMaterial with FAIRChem"},{"content":"By downloading content from LeMaterial, you agree to accept the Creative Commons Attribution 4.0 license implying that Content may be copied, distributed, transmitted, and adapted, without obtaining specific permission from LeMaterial, provided proper attribution is given to LeMaterial. If you use the LeMaterial as a resource in your research, please cite the citation section from our data-card (paper to come). CC-BY-4.0 requires proper acknowledgement.\nThus, If you use materials data which include (”mp-”) in the immutable_id, please cite the Materials Project.\nIf you use materials data which include (”agm-”) in the immutable_id, please cite Alexandria PBE or Alexandria PBESol, SCAN.\nIf you use materials data which include (”oqmd-”) in the immutable_id, please cite OQMD.\nFinally, if you make use of the Phase Diagram for visualization purposes, or the crystal viewer in the Materials Explorer, please acknowledge Crystal Toolkit.\n","date":"2023-09-07","id":4,"permalink":"/docs/about/citations/","summary":"\u003cp\u003eBy downloading content from LeMaterial, you agree to accept the Creative Commons Attribution 4.0 license implying that Content may be copied, distributed, transmitted, and adapted, without obtaining specific permission from LeMaterial, provided proper attribution is given to LeMaterial. If you use the LeMaterial as a resource in your research, please cite the \u003ca href=\"https://huggingface.co/datasets/LeMaterial/LeMat-Bulk#citation-information\"\u003ecitation section\u003c/a\u003e from our data-card (paper to come). \u003ca href=\"https://creativecommons.org/licenses/by/4.0/\"\u003eCC-BY-4.0\u003c/a\u003e requires proper acknowledgement.\u003c/p\u003e","tags":[],"title":"Citations"},{"content":"We are excited to invite AI practitioners from diverse backgrounds to join LeMaterial! Note that LeMaterial is a research collaboration and is open to participants who have\na professional research background are able to commit time to the project. In general, we expect applicants to be affiliated with a research organization (either in academia or industry) and work on the technical/ethical/legal aspects of LLMs for coding applications.\nYou can apply here to LeMaterial! You may also join our Slack channel for communication here. Additionally, you may add our Google calendar here. We will have monthly discussions regarding features, bugs and new development. Discussions will be held on the second Thursday of the Month at 6pm Paris time (9am Los Angeles time). The next event will be on February 13th.\n","date":"2023-09-07","id":5,"permalink":"/docs/about/how-to-join/","summary":"\u003cp\u003eWe are excited to invite AI practitioners from diverse backgrounds to join LeMaterial! Note that LeMaterial is a research collaboration and is open to participants who have\u003c/p\u003e","tags":[],"title":"How to join?"},{"content":"Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\nOur Standards Examples of behavior that contributes to a positive environment for our community include:\nDemonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include:\nThe use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others\u0026rsquo; private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\nScope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nEnforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at contact@lematerial.org. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\nEnforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n1. Correction Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n2. Warning Community Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n3. Temporary Ban Community Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n4. Permanent Ban Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\nAttribution This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla\u0026rsquo;s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.\n","date":"2023-09-07","id":6,"permalink":"/docs/about/code-of-conduct/","summary":"\u003ch2 id=\"our-pledge\"\u003eOur Pledge\u003c/h2\u003e\n\u003cp\u003eWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\u003c/p\u003e","tags":[],"title":"Code of Conduct"},{"content":"","date":"2025-02-06","id":7,"permalink":"/docs/learn/","summary":"","tags":[],"title":"Tutorials"},{"content":"","date":"2023-09-07","id":8,"permalink":"/docs/","summary":"","tags":[],"title":"About the project"},{"content":"","date":"2023-09-07","id":9,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"","date":"2023-09-07","id":10,"permalink":"/","summary":"","tags":[],"title":"LeMaterial"},{"content":"","date":"0001-01-01","id":11,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"0001-01-01","id":12,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"},{"content":"","date":"0001-01-01","id":13,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"}]